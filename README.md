# XOR Gate Implementation Using NumPy and TensorFlow

## Table of Contents
- [Introduction](#introduction)
- [Manual Implementation](#manual-implementation)
- [TensorFlow Implementation](#tensorflow-implementation)
- [How It Works](#how-it-works)
- [Installation](#installation)
- [Usage](#usage)
- [Results](#results)
- [Why This Matters](#why-this-matters)

---

## ğŸ“ Introduction

This repository demonstrates two ways to implement an **XOR gate** using **Neural Networks**:
1. **A handcrafted implementation using NumPy** that mimics logic gates.
2. **A trainable neural network using TensorFlow/Keras** that learns XOR through training.

The XOR function is a fundamental example of **non-linearly separable** problems, making it an ideal starting point for understanding neural networks.

---

## âš™ï¸ Manual Implementation

The first approach manually simulates an XOR gate using logic gates implemented with the **sigmoid activation function**. 

### ğŸ— How It Works:
- **Logic Gates:** XOR can be created using **NAND, OR, and AND** gates.
- **Sigmoid Activation:** Used to approximate these logic gates.
- **Layered Computation:** The first layer computes OR and NAND, and the second layer applies an AND operation.

### ğŸ“Œ Mathematical Representation:
Each gate follows the equation:
\[
f(x_1, x_2) = \sigma(w_1 \cdot x_1 + w_2 \cdot x_2 + b)
\]
where:
- \( \sigma(z) = \frac{1}{1 + e^{-z}} \) (Sigmoid function)
- \( w_1, w_2 \) are weights
- \( b \) is a bias term

### âœ… XOR Truth Table:
| x1 | x2 | XOR Output |
|----|----|-----------|
| 0  | 0  | 0         |
| 0  | 1  | 1         |
| 1  | 0  | 1         |
| 1  | 1  | 0         |

---

## ğŸ¤– TensorFlow Implementation

The second approach uses **TensorFlow/Keras** to build a **feedforward neural network** that learns XOR through training.

### ğŸ— Neural Network Architecture:
- **Input Layer:** Two neurons (for x1 and x2).
- **Hidden Layer:** Two neurons with **ReLU activation**.
- **Output Layer:** One neuron with **Sigmoid activation**.

### ğŸ‹ï¸ Training Details:
- **Loss Function:** Binary Crossentropy
- **Optimizer:** Adam
- **Epochs:** 1000

After training, the model learns to correctly predict the XOR function.

---

## ğŸ”¬ How It Works

### Manual XOR Logic:
1. Compute **OR** and **NAND** of inputs.
2. Use an **AND** gate to combine these results.
3. The final output follows the XOR pattern.

### TensorFlow XOR Learning:
1. The model initializes with **random weights**.
2. It **adjusts weights** using backpropagation and gradient descent.
3. Over **1000 training epochs**, it learns the XOR pattern.

---

## ğŸ›  Installation

Ensure you have the required dependencies installed:

```bash
pip install numpy tensorflow

## â–¶ï¸ Usage

Run the script to execute both implementations:

```bash
python xor_gate.py


## ğŸ“Š Results

After running the script, you should see:

### ğŸ›  Manual Implementation:
```yaml
XOR Gate Truth Table:
0, 0: 0
0, 1: 1
1, 0: 1
1, 1: 0


## ğŸ¤– TensorFlow Model Predictions:

XOR Predictions:
[0. 0.] â†’ 0 (expected: 0)
[0. 1.] â†’ 1 (expected: 1)
[1. 0.] â†’ 1 (expected: 1)
[1. 1.] â†’ 0 (expected: 0)


## ğŸŒŸ Why This Matters

XOR is non-linearly separable, requiring a multi-layered neural network.

Logic gates can be approximated using sigmoid activation.

Deep learning provides an automated way to learn these patterns instead of hardcoding logic.

Understanding XOR helps in learning more complex neural network architectures.


